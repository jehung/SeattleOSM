{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Wrangling: Seattle Open Street Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we analyze Open Street Map for the City of Seattle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore a Subset of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the dataset, we need a way to systematically slice the original dataset for a workable sample to explore. To this end, I have used the following code to achieve this. The **k** value is changed from large to small so that my resulting \n",
    "*SAMPLE_FILE* ends up at different sizes. When starting out, try using a larger k, then move on to an intermediate k before processing your whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OSM_FILE = \"seattle_washington.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"test.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5000 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'.encode('utf-8'))\n",
    "    output.write('<osm>\\n  '.encode('utf-8'))\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the above code, we end up with a file *test.osm* with which we can use to explore the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a Dictionary for All Tags In the Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to end up with a Python dictionary for the tags in the original dataset, so that we know what needs to be wrangled in the data. The following achives this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    \n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tags:\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] += 1\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = count_tags('seattle_washington.osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tags.pickle', 'wb') as tagsPickle:\n",
    "    pickle.dump(tags, tagsPickle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tags.pickle', 'rb') as tagsPickle:\n",
    "    unserialized_tags = pickle.load(tagsPickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 88068,\n",
       " 'nd': 8453162,\n",
       " 'node': 7580046,\n",
       " 'osm': 1,\n",
       " 'relation': 9411,\n",
       " 'tag': 4708553,\n",
       " 'way': 750242}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unserialized_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring What Is Contained Within Each Tag Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better sense of what sort of attributes is contained inside each type of tag, we use the following code to return this information to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds_subtags = []\n",
    "member_subtags = []\n",
    "nd_subtags = []\n",
    "node_subtags = []\n",
    "osm_subtags = []\n",
    "relation_subtags = []\n",
    "tag_subtags = []\n",
    "way_subtags = []\n",
    "\n",
    "for _, element in ET.iterparse('seattle_washington.osm'):\n",
    "    if element.tag == 'bounds' and element.attrib.keys() not in bounds_subtags:\n",
    "        bounds_subtags.append(element.attrib.keys())\n",
    "    elif element.tag == 'member' and element.attrib.keys() not in member_subtags:\n",
    "        member_subtags.append(element.attrib.keys())\n",
    "    elif element.tag == 'nd' and element.attrib.keys() not in nd_subtags:\n",
    "        nd_subtags.append(element.attrib.keys())\n",
    "    elif element.tag == 'node' and element.attrib.keys() not in node_subtags:\n",
    "        node_subtags.append(element.attrib.keys())\n",
    "    elif element.tag == 'osm' and element.attrib.keys() not in osm_subtags:\n",
    "        osm_subtags.append(element.attrib.keys())\n",
    "    elif element.tag == 'relation' and element.attrib.keys() not in relation_subtags:\n",
    "        relation_subtags.append(element.attrib.keys())\n",
    "    elif element.tag == 'tag' and element.attrib.keys() not in tag_subtags:\n",
    "        tag_subtags.append(element.attrib.keys())\n",
    "    elif element.tag == 'way' and element.attrib.keys() not in way_subtags:\n",
    "        way_subtags.append(element.attrib.keys())\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['maxlon', 'minlat', 'maxlat', 'minlon'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds_subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['type', 'role', 'ref'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "member_subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['ref'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd_subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['lat', 'user', 'uid', 'id', 'timestamp', 'lon', 'changeset', 'version']),\n",
       " dict_keys(['lat', 'id', 'timestamp', 'lon', 'changeset', 'version'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['timestamp', 'generator', 'version'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['id', 'user', 'changeset', 'timestamp', 'uid', 'version'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['id', 'user', 'changeset', 'timestamp', 'uid', 'version'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "way_subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['k', 'v'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_subtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, we started to get a sense of that the *xml* document was indeed used as a data storage tool, in that each of the elements in the above lists represents an **attribute** of an tag. Our data wrangling goal then, is to transform the information embedded in the OSM xml document into a JSON document with a flexible schema. In general, this schema will reflect the Python data structure of a dictionary, where an **attribute** is used as a **key**, and the **value of the attribute** is th corresponding **value** of the dictionary. \n",
    "\n",
    "Since a JSON document can be arbitrarily complex, it is an excellent fit for MongoDB, whose greatest feature is its flexible schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered in Seattle OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating several smallish subsets of Metro Seattle open street map xml files, I noticed the following problems with the data:\n",
    "\n",
    "* Inconsistent representation of street name (original name and abbreviated names):\n",
    "  Examples of this problem are King George Boulevard and also be written as King George Blvd, and Granville Street can also show up as Granville St or Granille St.\n",
    "\n",
    "* Inconsistent representation of addresses under the **tag** tag:\n",
    "  Address can either be represented by a many subtags, each with a key of **k='addr:city'**, **k='addr:street'**, **k='addr:postcode'**, **k='addr:housenumber'**. However, at times, an address can also be expressed as **k='addr:'**. \n",
    "  \n",
    "We now proceed with the address cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit Plan: Addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a visual inspection of the subset of Seattle OSM, we understood that **tag** contains address information. In particular, tags with attribute of  **k** of **addr:street** contains street names that tend to be described inconsistently in the dataset. Therefore, our next goal is to develop a data audit plan that works specifically on tags with addresses.\n",
    "\n",
    "The following chuncks of code achive this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit(osmfile):\n",
    "    osm_file = open('seattle_washington.osm', 'r', encoding='cp1252', errors='replace')\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=('start',)):\n",
    "        if elem.tag == 'node' or elem.tag == 'way':\n",
    "            for tag in elem.iter('tag'):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_types = audit(OSM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Python dictionary shows the entire collection of street types after we have done our initial cleaning. Now we see that a vast majority of street types no longer bears problems. However, some of the street types are obviously wrong. Most notably, whenever **Suite number / apartment number** is present in the street name, the code has confused it with the name of the street. This needs our attention.\n",
    "\n",
    "To address this problem, we should make sure that our subsequent code to clean and wrangle the OSM data will shape the raw data in such a way that will avoid confusing the suite number / apartment number with the street name. A convenient way to achieve this is to present an address in the JSON document (namely, the cleaned file) with schema such as this:\n",
    "\n",
    "\"address\": {\"street\": \"3401 Evanston Ave N, Suite A\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave.\": \"Avenue\"\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    street_type = m.group()\n",
    "    \n",
    "    name = re.sub(street_type, mapping[street_type], name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ave': 'Avenue', 'Rd.': 'Road', 'St': 'Street', 'St.': 'Street'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for Dababase Insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can describe our audit plan so far as this:\n",
    "\n",
    "* First we explored the variations and types of street name representation found in the Seattle OSM data. Then we normalized the abbreviated street type representations.\n",
    "\n",
    "* Second transform the key-value pairs found in the **tag** tag as described earlier.\n",
    "\n",
    "* Finally, we will normalize the overall address representation by removing the tags with attribute **addr:**, and make sure that our final **street** key will show the street name as well as the suite / apartment number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code below, then we are ready to insert the cleaned and wrangled data into the MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "import collections\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "address_regex = re.compile(r'^addr\\:')\n",
    "street_regex = re.compile(r'^street')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    position_attributes = ['lat', 'lon']\n",
    "    created_attributes = CREATED\n",
    "\n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        # populate tag type\n",
    "        node['type'] = element.tag\n",
    "\n",
    "        # initialize address\n",
    "        address = {}\n",
    "\n",
    "        # parse through attributes\n",
    "        for attribute in element.attrib:\n",
    "            if attribute in CREATED:\n",
    "                if 'created' not in node:\n",
    "                    node['created'] = {}\n",
    "                node['created'][attribute] = element.get(attribute)\n",
    "            elif attribute in position_attributes:\n",
    "                continue\n",
    "            else:\n",
    "                node[attribute] = element.get(attribute)\n",
    "\n",
    "        # populate position\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = [float(element.get('lat')), float(element.get('lon'))]\n",
    "\n",
    "        # parse second-level tags for nodes\n",
    "        for child in element:\n",
    "            # parse second-level tags for ways and populate `node_refs`\n",
    "            if child.tag == 'nd':\n",
    "                if 'node_refs' not in node:\n",
    "                    node['node_refs'] = []\n",
    "                if 'ref' in child.attrib:\n",
    "                    node['node_refs'].append(child.get('ref'))\n",
    "\n",
    "            # throw out not-tag elements and elements without `k` or `v`\n",
    "            if child.tag != 'tag'\\\n",
    "            or 'k' not in child.attrib\\\n",
    "            or 'v' not in child.attrib:\n",
    "                continue\n",
    "            key = child.get('k')\n",
    "            val = child.get('v')\n",
    "\n",
    "            # skip problematic characters\n",
    "            if problemchars.search(key):\n",
    "                continue\n",
    "\n",
    "            # parse address k-v pairs\n",
    "            elif address_regex.search(key):\n",
    "                key = key.replace('addr:', '')\n",
    "                address[key] = val\n",
    "\n",
    "\n",
    "            # catch-all\n",
    "            else:\n",
    "                node[key] = val\n",
    "        # compile address\n",
    "        if len(address) > 0:\n",
    "            node['address'] = {}\n",
    "            street_full = None\n",
    "            street_dict = {}\n",
    "            street_format = ['prefix', 'name', 'type']\n",
    "            # parse through address objects\n",
    "            for key in address:\n",
    "                val = address[key]\n",
    "                if street_regex.search(key):\n",
    "                    if key == 'street':\n",
    "                        street_full = update_name(val, mapping) if val in mapping else val\n",
    "                    elif 'street:' in key:\n",
    "                        street_dict[key.replace('street:', '')] = update_name(val, mapping) if val in mapping else val\n",
    "                else:\n",
    "                    node['address'][key] = update_name(val, mapping) if val in mapping else val\n",
    "            # assign street_full or fallback to compile street dict\n",
    "            if street_full:\n",
    "                node['address']['street'] = update_name(street_full, mapping) if street_full in mapping else street_full\n",
    "            elif len(street_dict) > 0:\n",
    "                node['address']['street'] = ' '.join([street_dict[key] for key in street_format])\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = process_map('seattle_washington.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Into MongoDB Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, we then insert into the MongoDB database using the following cmd command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mongoimport -host 127.0.0.1:27017 --db osm --collection seattle_osm --drop --file C:\\Users\\Jenny\\Documents\\Mathfreak_Data\\School\\Data_Analysis_ND\\Project3\\seattle_washingotn.osm.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring the Database in MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "seattle_washingotn.osm ......... 1,649 MB\n",
    "seattle_washingotn.osm.json .... 1,870 MB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Seattle Area Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def get_db():\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client.osm\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8330288"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = get_db()\n",
    "db.seattle_osm.find().count()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that once cleaned and imported, the Seattle OSM collection has approximately 8.3 million data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7580018"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.seattle_osm.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Seattle OSM collection has roughly 7.5 million nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750175"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.seattle_osm.find({\"type\":\"way\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.seattle_osm.distinct(\"created.user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Amenities in Metro Seattle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amenity_result = db.seattle_osm.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}}}, {\"$group\":{\"_id\":\"$amenity\",\"count\":{\"$sum\":1}}}, \n",
    "                          {\"$sort\":{\"count\":-1}}, {\"$limit\":10}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 10753, '_id': 'parking'}\n",
      "{'count': 3318, '_id': 'bicycle_parking'}\n",
      "{'count': 3293, '_id': 'restaurant'}\n",
      "{'count': 2848, '_id': 'bench'}\n",
      "{'count': 2458, '_id': 'school'}\n",
      "{'count': 1645, '_id': 'place_of_worship'}\n",
      "{'count': 1578, '_id': 'fast_food'}\n",
      "{'count': 1499, '_id': 'cafe'}\n",
      "{'count': 1294, '_id': 'waste_basket'}\n",
      "{'count': 1153, '_id': 'fuel'}\n"
     ]
    }
   ],
   "source": [
    "for a in amenity_result:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Place of Worship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "religion_result = db.seattle_osm.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"place_of_worship\"}},\n",
    "                                                {\"$group\":{\"_id\":\"$religion\", \"count\":{\"$sum\":1}}},\n",
    "                                                {\"$sort\":{\"count\":-1}}, {\"$limit\":10}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1500, '_id': 'christian'}\n",
      "{'count': 80, '_id': None}\n",
      "{'count': 20, '_id': 'jewish'}\n",
      "{'count': 17, '_id': 'buddhist'}\n",
      "{'count': 7, '_id': 'muslim'}\n",
      "{'count': 6, '_id': 'unitarian_universalist'}\n",
      "{'count': 3, '_id': 'sikh'}\n",
      "{'count': 2, '_id': 'bahai'}\n",
      "{'count': 2, '_id': 'eckankar'}\n",
      "{'count': 2, '_id': 'spiritualist'}\n"
     ]
    }
   ],
   "source": [
    "for r in religion_result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Dining Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dinning_result = db.seattle_osm.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"restaurant\"}}, \n",
    "                          {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},        \n",
    "                          {\"$sort\":{\"count\":-1}}, {\"$limit\":10}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 817, '_id': None}\n",
      "{'count': 265, '_id': 'mexican'}\n",
      "{'count': 257, '_id': 'pizza'}\n",
      "{'count': 249, '_id': 'american'}\n",
      "{'count': 162, '_id': 'asian'}\n",
      "{'count': 155, '_id': 'thai'}\n",
      "{'count': 150, '_id': 'chinese'}\n",
      "{'count': 123, '_id': 'japanese'}\n",
      "{'count': 117, '_id': 'italian'}\n",
      "{'count': 101, '_id': 'burger'}\n"
     ]
    }
   ],
   "source": [
    "for d in dinning_result:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Types of Coffee Shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cafe_result = db.seattle_osm.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"cafe\"}}, \n",
    "                          {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},        \n",
    "                          {\"$sort\":{\"count\":-1}}, {\"$limit\":10}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 663, '_id': 'coffee_shop'}\n",
      "{'count': 567, '_id': None}\n",
      "{'count': 48, '_id': 'ice_cream'}\n",
      "{'count': 20, '_id': 'sandwich'}\n",
      "{'count': 19, '_id': 'american'}\n",
      "{'count': 15, '_id': 'tea'}\n",
      "{'count': 14, '_id': 'donut;coffee_shop'}\n",
      "{'count': 10, '_id': 'vietnamese'}\n",
      "{'count': 10, '_id': 'donut'}\n",
      "{'count': 9, '_id': 'frozen_yogurt'}\n"
     ]
    }
   ],
   "source": [
    "for c in cafe_result:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Distinct Points Contained in Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above data view showed that there are oughly 8.3 million data points, it does not directly address the question of how many distinct geographical points does Seattle OSM contain. To answer this question, we go about the route of usign the geo position of latitue and longitude, together with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pipeline():\n",
    "    pipeline = [ ]\n",
    "    group = {'$group':{'_id':'$pos', 'uniq_count': { '$sum': 1 }}}\n",
    "    sort = {'$sort':{'count':-1}}\n",
    "    group1 = {'$count':'uniq_count'}\n",
    "    \n",
    "    for e in [group, sort, group1]:\n",
    "        pipeline.append(e)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.seattle_osm.aggregate(pipeline)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that given the size of our returned query result, we have to write the results into a collection, and iterate through each element to get the detailed results contained withitn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline()\n",
    "pos_result = db.seattle_osm.aggregate(pipeline, allowDiskUse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in pos_result:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Seattle OSM contained roughly 7.5 million distinct geographical \"points\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I discovered that the Seatlte OSM data had inadvertently contained some points that actually does **not** belong to Seattle at all - in fact, they belong to a neighboring city in a neighboring country, Victoria, Canada. This was probably due to the works for GPS systems that categorized these points based on their vicinity to Seattle. Indeed, Victoria is closer to Seattle than to Vancouver, BC. \n",
    "\n",
    "This problem was discovered using this following exploration technique by looking at the name of the cities that were found in the dataset -- sorted in ascending order. While doing this exploration, all of the cities are indeed within Washington state, except for Victoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_result = db.seattle_osm.aggregate([{\"$match\":{\"address.city\":{\"$exists\":1}}}, \n",
    "                   {\"$group\":{\"_id\":\"$address.city\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":1}}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1, '_id': 'Rainier'}\n",
      "{'count': 1, '_id': 'Port Angles'}\n",
      "{'count': 1, '_id': 'Centalia'}\n",
      "{'count': 1, '_id': 'everett'}\n",
      "{'count': 1, '_id': 'Silvana'}\n",
      "{'count': 1, '_id': 'Aberdeen, WA'}\n",
      "{'count': 1, '_id': 'Roy'}\n",
      "{'count': 1, '_id': 'Clyde Hill'}\n",
      "{'count': 1, '_id': 'BOTHELL'}\n",
      "{'count': 1, '_id': 'RENTON'}\n",
      "{'count': 1, '_id': 'Picnic Point-North Lynnwood'}\n",
      "{'count': 1, '_id': 'Greenbank (Whidbey Island)'}\n",
      "{'count': 1, '_id': 'Tumwater, WA'}\n",
      "{'count': 1, '_id': 'Cottage Lake'}\n",
      "{'count': 1, '_id': 'McChord Field'}\n",
      "{'count': 1, '_id': 'Graham'}\n",
      "{'count': 1, '_id': 'Meridian'}\n",
      "{'count': 1, '_id': 'Shelton, WA'}\n",
      "{'count': 1, '_id': 'Olympia, WA '}\n",
      "{'count': 1, '_id': 'redmond'}\n",
      "{'count': 1, '_id': 'bainbridge Island'}\n",
      "{'count': 1, '_id': 'Oting'}\n",
      "{'count': 1, '_id': 'Newcastle, WA'}\n",
      "{'count': 1, '_id': 'port Townsend'}\n",
      "{'count': 1, '_id': 'Bothel'}\n",
      "{'count': 1, '_id': 'Clearlake'}\n",
      "{'count': 1, '_id': 'Yelm, WA'}\n",
      "{'count': 1, '_id': 'Mt.Vernon;Mount Vernon'}\n",
      "{'count': 1, '_id': 'goldbar'}\n",
      "{'count': 1, '_id': 'Olympia, WA'}\n",
      "{'count': 1, '_id': 'Yelm'}\n",
      "{'count': 1, '_id': 'Woodenville'}\n",
      "{'count': 1, '_id': 'Friday Harbor'}\n",
      "{'count': 1, '_id': 'BELLEVUE'}\n",
      "{'count': 1, '_id': 'Hansville'}\n",
      "{'count': 1, '_id': 'Packwood'}\n",
      "{'count': 1, '_id': 'Oakville'}\n",
      "{'count': 1, '_id': 'EVERETT'}\n",
      "{'count': 1, '_id': 'Belevue'}\n",
      "{'count': 1, '_id': 'roy'}\n",
      "{'count': 1, '_id': 'Port Ludlow'}\n",
      "{'count': 1, '_id': 'Sultan'}\n",
      "{'count': 1, '_id': 'Maltby'}\n",
      "{'count': 1, '_id': 'Intercity Transit'}\n",
      "{'count': 1, '_id': 'kenmore'}\n",
      "{'count': 1, '_id': 'Longbranch'}\n",
      "{'count': 1, '_id': 'Concrete'}\n",
      "{'count': 1, '_id': 'Marysville, WA'}\n",
      "{'count': 1, '_id': 'tukwila'}\n",
      "{'count': 1, '_id': 'Brier'}\n",
      "{'count': 1, '_id': 'Lake Stephens'}\n",
      "{'count': 1, '_id': 'Remond'}\n",
      "{'count': 1, '_id': 'Crystal Mountain'}\n",
      "{'count': 1, '_id': 'Littlerock'}\n",
      "{'count': 1, '_id': 'SILVERDALE'}\n",
      "{'count': 1, '_id': 'Wilkeson'}\n",
      "{'count': 1, '_id': 'Mt. Vernon'}\n",
      "{'count': 1, '_id': 'Kirkalnd'}\n",
      "{'count': 1, '_id': 'South Bend'}\n",
      "{'count': 1, '_id': 'Coupeville (Whidbey Island)'}\n",
      "{'count': 2, '_id': 'coupeville'}\n",
      "{'count': 2, '_id': 'Fox Island'}\n",
      "{'count': 2, '_id': 'gig Harbor'}\n",
      "{'count': 2, '_id': 'Raymond'}\n",
      "{'count': 2, '_id': 'Lake Tapps'}\n",
      "{'count': 2, '_id': 'Littlerock, Washington'}\n",
      "{'count': 2, '_id': 'Oak Harbor (Whidbey Island)'}\n",
      "{'count': 2, '_id': 'Grand Mound'}\n",
      "{'count': 2, '_id': 'Shelton'}\n",
      "{'count': 2, '_id': 'Hamilton'}\n",
      "{'count': 2, '_id': 'Snoqualmie Pass'}\n",
      "{'count': 2, '_id': 'Seattle, WA'}\n",
      "{'count': 2, '_id': 'olympia'}\n",
      "{'count': 2, '_id': 'Lacy'}\n",
      "{'count': 2, '_id': 'woodinville'}\n",
      "{'count': 2, '_id': 'anacortes'}\n",
      "{'count': 2, '_id': 'Quilcene'}\n",
      "{'count': 2, '_id': 'Conway'}\n",
      "{'count': 2, '_id': 'Hoquiam'}\n",
      "{'count': 2, '_id': 'Tumwater, Washington'}\n",
      "{'count': 2, '_id': 'Yarrow Point'}\n",
      "{'count': 2, '_id': 'lakewood'}\n",
      "{'count': 2, '_id': 'Greenwater'}\n",
      "{'count': 2, '_id': 'Elma'}\n",
      "{'count': 2, '_id': 'Sequim'}\n",
      "{'count': 2, '_id': 'marysville'}\n",
      "{'count': 2, '_id': 'Edmonds, WA'}\n",
      "{'count': 3, '_id': 'Indianola'}\n",
      "{'count': 3, '_id': 'issaquah'}\n",
      "{'count': 3, '_id': 'Pacific'}\n",
      "{'count': 3, '_id': 'Brinnon'}\n",
      "{'count': 3, '_id': 'Keyport'}\n",
      "{'count': 3, '_id': 'Enumclaw'}\n",
      "{'count': 3, '_id': 'Suquamish'}\n",
      "{'count': 3, '_id': 'Cosmopolis'}\n",
      "{'count': 3, '_id': 'McCleary, WA'}\n",
      "{'count': 3, '_id': 'Preston'}\n",
      "{'count': 3, '_id': 'Dupont'}\n",
      "{'count': 3, '_id': 'Ashford'}\n",
      "{'count': 3, '_id': 'tacoma'}\n",
      "{'count': 3, '_id': 'Clear Lake'}\n",
      "{'count': 3, '_id': 'Steilacoom'}\n",
      "{'count': 3, '_id': 'Sedro-Woolley'}\n",
      "{'count': 3, '_id': 'Index'}\n",
      "{'count': 4, '_id': 'Buckley'}\n",
      "{'count': 4, '_id': 'Lopez Island'}\n",
      "{'count': 4, '_id': 'Vashon Island'}\n",
      "{'count': 4, '_id': 'Spanaway'}\n",
      "{'count': 4, '_id': 'Carnation'}\n",
      "{'count': 4, '_id': 'Port Gamble'}\n",
      "{'count': 4, '_id': 'Sumner'}\n",
      "{'count': 4, '_id': 'Rockport'}\n",
      "{'count': 4, '_id': 'rochester'}\n",
      "{'count': 4, '_id': 'Seatac'}\n",
      "{'count': 4, '_id': 'Tumwater'}\n",
      "{'count': 4, '_id': 'Black Diamond'}\n",
      "{'count': 4, '_id': 'Granite Falls'}\n",
      "{'count': 5, '_id': 'Marblemount'}\n",
      "{'count': 5, '_id': 'Lyman'}\n",
      "{'count': 5, '_id': 'seattle'}\n",
      "{'count': 5, '_id': 'Bow'}\n",
      "{'count': 5, '_id': 'Skykomish'}\n",
      "{'count': 5, '_id': 'Tulalip'}\n",
      "{'count': 5, '_id': 'Camano Island'}\n",
      "{'count': 6, '_id': 'Bonney Lake'}\n",
      "{'count': 6, '_id': 'Chimacum'}\n",
      "{'count': 7, '_id': 'Seabeck'}\n",
      "{'count': 7, '_id': 'Covington'}\n",
      "{'count': 7, '_id': 'Chehalis'}\n",
      "{'count': 7, '_id': 'Mercer Island'}\n",
      "{'count': 7, '_id': 'University Place'}\n",
      "{'count': 7, '_id': 'DuPont'}\n",
      "{'count': 7, '_id': 'Clinton'}\n",
      "{'count': 8, '_id': 'Edgewood'}\n",
      "{'count': 8, '_id': 'Eatonville'}\n",
      "{'count': 8, '_id': 'Vashon'}\n",
      "{'count': 8, '_id': 'Lake Stevens'}\n",
      "{'count': 9, '_id': 'Duvall'}\n",
      "{'count': 9, '_id': 'Lake Forest Park'}\n",
      "{'count': 10, '_id': 'SeaTac'}\n",
      "{'count': 10, '_id': 'Oak Harbor'}\n",
      "{'count': 10, '_id': 'Mountlake Terrace'}\n",
      "{'count': 11, '_id': 'Aberdeen'}\n",
      "{'count': 11, '_id': 'Fall City'}\n",
      "{'count': 13, '_id': 'Snohomish'}\n",
      "{'count': 14, '_id': 'Lynwood'}\n",
      "{'count': 14, '_id': 'Monroe'}\n",
      "{'count': 16, '_id': 'Tukwila'}\n",
      "{'count': 16, '_id': 'Mukilteo'}\n",
      "{'count': 17, '_id': 'Newcastle'}\n",
      "{'count': 17, '_id': 'Maple Valley'}\n",
      "{'count': 17, '_id': 'Silverdale'}\n",
      "{'count': 17, '_id': 'Coupeville'}\n",
      "{'count': 18, '_id': 'New Songhees 1A'}\n",
      "{'count': 18, '_id': 'Olympia, Washington'}\n",
      "{'count': 21, '_id': \"T'Sou-ke\"}\n",
      "{'count': 21, '_id': 'Frederickson'}\n",
      "{'count': 21, '_id': 'Stanwood'}\n",
      "{'count': 27, '_id': 'La Conner'}\n",
      "{'count': 29, '_id': 'Bellevue, WA'}\n",
      "{'count': 30, '_id': 'Becher Bay 1'}\n",
      "{'count': 32, '_id': 'Lakewood'}\n",
      "{'count': 32, '_id': 'Port Angeles'}\n",
      "{'count': 33, '_id': 'North Bend'}\n",
      "{'count': 34, '_id': 'Port Townsend'}\n",
      "{'count': 37, '_id': 'Langley'}\n",
      "{'count': 37, '_id': 'Milton'}\n",
      "{'count': 39, '_id': 'Auburn'}\n",
      "{'count': 39, '_id': 'Snoqualmie'}\n",
      "{'count': 40, '_id': 'Arlington'}\n",
      "{'count': 41, '_id': 'Kenmore'}\n",
      "{'count': 42, '_id': 'Fife'}\n",
      "{'count': 42, '_id': 'Elbe'}\n",
      "{'count': 43, '_id': 'Freeland'}\n",
      "{'count': 43, '_id': 'Marysville'}\n",
      "{'count': 50, '_id': 'Shoreline'}\n",
      "{'count': 55, '_id': 'Burien'}\n",
      "{'count': 57, '_id': 'Anacortes'}\n",
      "{'count': 58, '_id': 'Sammamish'}\n",
      "{'count': 61, '_id': 'Lacey'}\n",
      "{'count': 71, '_id': 'Olympia'}\n",
      "{'count': 73, '_id': 'Carbonado'}\n",
      "{'count': 79, '_id': 'Bremerton'}\n",
      "{'count': 80, '_id': 'Lynnwood'}\n",
      "{'count': 85, '_id': 'Burlington'}\n",
      "{'count': 104, '_id': 'Poulsbo'}\n",
      "{'count': 104, '_id': 'Port Orchard'}\n",
      "{'count': 106, '_id': 'Sedro Woolley'}\n",
      "{'count': 119, '_id': 'Bainbridge Island'}\n",
      "{'count': 126, '_id': 'Everett'}\n",
      "{'count': 138, '_id': 'Federal Way'}\n",
      "{'count': 159, '_id': 'Des Moines'}\n",
      "{'count': 186, '_id': 'Kent'}\n",
      "{'count': 194, '_id': 'Hunts Point'}\n",
      "{'count': 214, '_id': 'Edmonds'}\n",
      "{'count': 215, '_id': 'Centralia'}\n",
      "{'count': 216, '_id': 'Issaquah'}\n",
      "{'count': 217, '_id': 'Bothell'}\n",
      "{'count': 227, '_id': 'Gig Harbor'}\n",
      "{'count': 235, '_id': 'Renton'}\n",
      "{'count': 255, '_id': 'Woodinville'}\n",
      "{'count': 356, '_id': 'Highlands'}\n",
      "{'count': 435, '_id': 'Redmond'}\n",
      "{'count': 470, '_id': 'Puyallup'}\n",
      "{'count': 490, '_id': 'Bellevue'}\n",
      "{'count': 513, '_id': 'Rochester'}\n",
      "{'count': 602, '_id': 'Kingston'}\n",
      "{'count': 623, '_id': 'Tacoma'}\n",
      "{'count': 729, '_id': 'Victoria'}\n",
      "{'count': 739, '_id': 'Capital H (Part 1)'}\n",
      "{'count': 759, '_id': 'Mill Creek'}\n",
      "{'count': 918, '_id': 'Metchosin'}\n",
      "{'count': 998, '_id': 'View Royal'}\n",
      "{'count': 1495, '_id': 'Esquimalt'}\n",
      "{'count': 1600, '_id': 'Sooke'}\n",
      "{'count': 1985, '_id': 'Colwood'}\n",
      "{'count': 2298, '_id': 'Oak Bay'}\n",
      "{'count': 2806, '_id': 'Langford'}\n",
      "{'count': 11547, '_id': 'Saanich'}\n",
      "{'count': 11733, '_id': 'Mount Vernon'}\n",
      "{'count': 42285, '_id': 'Kirkland'}\n",
      "{'count': 203375, '_id': 'Seattle'}\n"
     ]
    }
   ],
   "source": [
    "for v in not_result:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more direct way of finding out if this particular Canadian city was somehow included in the Seattle OSM data file, we use this folloing code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vic_result = db.seattle_osm.aggregate([{\"$match\":{\"address.city\":{\"$eq\":\"Victoria\"}}}, \n",
    "                   {\"$group\":{\"_id\":\"$address.city\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":1}}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 729, '_id': 'Victoria'}\n"
     ]
    }
   ],
   "source": [
    "for v in vic_result:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this review, it would appear that the Seattle OSM data set had leveraged some automatic geo-data collection techniques such as GPS devices while a vehicle is in motion. Although some of the datapoints clearly do not belong to Seattle metro area, I now feel that as a while, the Seatlte OSM has been cleaned sufficiently for the purposes of this exercise. \n",
    "\n",
    "As a further note of possible future research, I find the proper identification of whether a geo location shoudl belong in the Seattle area can be made better by taking advantage of some elementary classification techniques using machine learning. This might be a more applicable point as more and more geographical data are gradually being collected by automatic means in the future."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
